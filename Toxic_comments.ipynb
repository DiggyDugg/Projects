{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Анализ-данных\" data-toc-modified-id=\"Анализ-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Анализ данных</a></span></li><li><span><a href=\"#Очиcтка-и-лемматизация\" data-toc-modified-id=\"Очиcтка-и-лемматизация-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Очиcтка и лемматизация</a></span></li><li><span><a href=\"#Подготовка-признаков\" data-toc-modified-id=\"Подготовка-признаков-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Подготовка признаков</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Catboost\" data-toc-modified-id=\"Catboost-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Catboost</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Необходимо построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ilyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ilyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ilyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ilyan\\projects\\Git\\Projects\\Toxic_comments\\toxic_comments.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Positive: 143346, Share:89.83%, Negative16225, Share: 10.17%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df['toxic'].unique())\n",
    "zero = df[df['toxic']==0]['toxic'].count()\n",
    "total = df.shape[0]\n",
    "display(f'Positive: {zero}, Share:{zero/total:0.2%}, Negative{total-zero}, Share: {1-zero/total:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Негативных сообщений меньше позитивных в 9 раз. При классификации будет полезно учесть этот дисбаланс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очиcтка и лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция очистки текста от лишних символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    s = re.sub(r\"[^a-zA-Z']\", ' ', text)\n",
    "    return \" \".join (s.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим функции и токенизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 40s\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww He match this background colour I'm seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I'm really not trying to edit war It's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can't make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  Explanation Why the edits made under my userna...  \n",
       "1  D'aww He match this background colour I'm seem...  \n",
       "2  Hey man I'm really not trying to edit war It's...  \n",
       "3  More I can't make any real suggestion on impro...  \n",
       "4  You sir are my hero Any chance you remember wh...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "df['text_lemmatized'] = df['text'].apply(lemmatize_text)\n",
    "df['text_lemmatized'] = df['text_lemmatized'].apply(clear_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target = df['toxic']\n",
    "corpus = df['text_lemmatized'].values\n",
    "\n",
    "corpus_train, corpus_test, target_train, target_test = train_test_split(corpus,target, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуем корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 57.6 s\n",
      "Wall time: 57.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "features_train = count_tf_idf.fit_transform(corpus_train)\n",
    "features_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (hh:mm:ss.ms) 0:15:43.103789\n",
      " Results from Grid Search \n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.7633855893150552\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'C': 2.976351441631316, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight = 'balanced', random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    'penalty' : ['l1','l2'], \n",
    "    'C'       : np.logspace(-3,3,20),\n",
    "    'solver'  : ['newton-cg', 'lbfgs', 'liblinear']\n",
    "}\n",
    "CV = 5\n",
    "    \n",
    "start_time = datetime.now()\n",
    "lr_GS = GridSearchCV(model, parameters, scoring = 'f1', cv=CV)\n",
    "lr_GS.fit(features_train,target_train)\n",
    "lr_GS_time_fit = datetime.now() - start_time\n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(lr_GS_time_fit))\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best score across ALL searched params:\\n\", lr_GS.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", lr_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.7691216120334895\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_GS.best_estimator_.predict(features_test)\n",
    "\n",
    "print('F1 =', f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем выборку, чтобы модель обучалась приемлемое время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(30000, random_state = 42).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим признаки для выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "df_sample['text_lemmatized'] = df_sample['text'].apply(lemmatize_text)\n",
    "df_sample['text_lemmatized'] = df_sample['text_lemmatized'].apply(clear_text)\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sample = df_sample['toxic']\n",
    "corpus_sample = df_sample['text_lemmatized'].values\n",
    "\n",
    "corpus_train_sample, corpus_val_sample, target_train_sample, target_val_sample = train_test_split(corpus_sample,target_sample, test_size=0.20, random_state=42)\n",
    "corpus_valid_sample, corpus_test_sample, target_valid_sample, target_test_sample = train_test_split(corpus_val_sample,target_val_sample, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.7 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "features_train_sample = count_tf_idf.fit_transform(corpus_train_sample)\n",
    "features_valid_sample = count_tf_idf.transform(corpus_valid_sample)\n",
    "features_test_sample = count_tf_idf.transform(corpus_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 55073)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.6666666666666666\n",
      "CPU times: total: 1h 45min 24s\n",
      "Wall time: 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = CatBoostClassifier(random_state=42,n_estimators=80, depth =11)\n",
    "clf.fit(features_train_sample,target_train_sample, eval_set=(features_valid_sample,target_valid_sample), verbose=False)\n",
    "predictions_cb = clf.predict(features_test_sample)\n",
    "f1_result = f1_score(target_test_sample, predictions_cb)\n",
    "print('F1 =', f1_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель логистической регрессии достигла значения 0,76, при этом  быстро обучилась на всем датафрейме. Catboost обучен на 24000 строк, что составляет 15% от всего датафрейма, с результатом 0,66. Вероятно, при обучении на большем объеме данных, модель достигла бы порога 0,75. "
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 4775,
    "start_time": "2022-05-28T21:40:45.486Z"
   },
   {
    "duration": 152,
    "start_time": "2022-05-28T21:41:34.261Z"
   },
   {
    "duration": 26,
    "start_time": "2022-05-28T21:41:43.486Z"
   },
   {
    "duration": 3194,
    "start_time": "2022-05-28T21:42:08.136Z"
   },
   {
    "duration": 230,
    "start_time": "2022-05-28T21:42:40.228Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T21:43:00.999Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T21:47:52.547Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T21:48:51.963Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T21:49:28.726Z"
   },
   {
    "duration": 36872,
    "start_time": "2022-05-28T21:50:39.015Z"
   },
   {
    "duration": 23,
    "start_time": "2022-05-28T21:51:25.294Z"
   },
   {
    "duration": 6791,
    "start_time": "2022-05-28T21:56:03.256Z"
   },
   {
    "duration": 49880,
    "start_time": "2022-05-28T21:56:32.621Z"
   },
   {
    "duration": 31,
    "start_time": "2022-05-28T22:02:37.818Z"
   },
   {
    "duration": 216107,
    "start_time": "2022-05-28T22:02:39.034Z"
   },
   {
    "duration": 965,
    "start_time": "2022-05-28T22:09:27.244Z"
   },
   {
    "duration": 912,
    "start_time": "2022-05-28T22:09:34.301Z"
   },
   {
    "duration": 12,
    "start_time": "2022-05-28T22:10:18.162Z"
   },
   {
    "duration": 201,
    "start_time": "2022-05-28T22:10:34.335Z"
   },
   {
    "duration": 207643,
    "start_time": "2022-05-28T22:10:41.308Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T22:17:02.716Z"
   },
   {
    "duration": 113,
    "start_time": "2022-05-28T22:17:48.104Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-28T22:17:48.218Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-28T22:17:48.219Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-28T22:17:48.220Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-28T22:17:48.222Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-28T22:17:48.223Z"
   },
   {
    "duration": 1358,
    "start_time": "2022-05-28T22:18:57.936Z"
   },
   {
    "duration": 893,
    "start_time": "2022-05-28T22:18:59.296Z"
   },
   {
    "duration": 27,
    "start_time": "2022-05-28T22:19:00.193Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-28T22:19:00.222Z"
   },
   {
    "duration": 9,
    "start_time": "2022-05-28T22:19:00.226Z"
   },
   {
    "duration": 37164,
    "start_time": "2022-05-28T22:19:00.236Z"
   },
   {
    "duration": 22,
    "start_time": "2022-05-28T22:19:37.401Z"
   },
   {
    "duration": 6863,
    "start_time": "2022-05-28T22:19:37.425Z"
   },
   {
    "duration": 47608,
    "start_time": "2022-05-28T22:19:44.289Z"
   },
   {
    "duration": 114,
    "start_time": "2022-05-28T22:20:31.900Z"
   },
   {
    "duration": 6521,
    "start_time": "2022-05-28T22:20:32.016Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-28T22:20:38.539Z"
   },
   {
    "duration": 1238,
    "start_time": "2022-05-28T22:20:38.554Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-28T22:20:39.794Z"
   },
   {
    "duration": 1083224,
    "start_time": "2022-05-28T22:20:39.799Z"
   },
   {
    "duration": 5126,
    "start_time": "2022-05-29T16:35:06.440Z"
   },
   {
    "duration": 877,
    "start_time": "2022-05-29T16:35:12.556Z"
   },
   {
    "duration": 43,
    "start_time": "2022-05-29T16:35:13.435Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-29T16:35:14.908Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-29T16:35:15.596Z"
   },
   {
    "duration": 125,
    "start_time": "2022-05-29T16:35:24.049Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-29T16:39:40.116Z"
   },
   {
    "duration": 15,
    "start_time": "2022-05-29T16:39:44.883Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-29T16:39:47.876Z"
   },
   {
    "duration": 12,
    "start_time": "2022-05-29T16:39:50.268Z"
   },
   {
    "duration": 1462,
    "start_time": "2022-05-29T16:44:30.243Z"
   },
   {
    "duration": 947,
    "start_time": "2022-05-29T16:44:31.708Z"
   },
   {
    "duration": 33,
    "start_time": "2022-05-29T16:44:32.657Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-29T16:44:32.692Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-29T16:44:32.698Z"
   },
   {
    "duration": 38516,
    "start_time": "2022-05-29T16:44:32.704Z"
   },
   {
    "duration": 17,
    "start_time": "2022-05-29T16:45:11.221Z"
   },
   {
    "duration": 7072,
    "start_time": "2022-05-29T16:45:11.240Z"
   },
   {
    "duration": 63247,
    "start_time": "2022-05-29T16:45:18.314Z"
   },
   {
    "duration": 54,
    "start_time": "2022-05-29T16:46:21.563Z"
   },
   {
    "duration": 8094,
    "start_time": "2022-05-29T16:46:21.619Z"
   },
   {
    "duration": 11,
    "start_time": "2022-05-29T16:46:29.714Z"
   },
   {
    "duration": 1406,
    "start_time": "2022-05-29T16:46:29.726Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-29T16:46:31.134Z"
   },
   {
    "duration": 87696,
    "start_time": "2022-05-29T16:46:31.139Z"
   },
   {
    "duration": 10950,
    "start_time": "2022-05-29T16:48:03.411Z"
   },
   {
    "duration": 10986,
    "start_time": "2022-05-29T16:48:21.674Z"
   },
   {
    "duration": 1003,
    "start_time": "2022-05-29T16:48:42.081Z"
   },
   {
    "duration": 10013,
    "start_time": "2022-05-29T16:48:46.052Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-29T16:49:09.082Z"
   },
   {
    "duration": 1154,
    "start_time": "2022-05-29T16:49:11.114Z"
   },
   {
    "duration": 11065,
    "start_time": "2022-05-29T16:49:14.595Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-29T16:50:00.026Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-29T16:50:03.077Z"
   },
   {
    "duration": 11423,
    "start_time": "2022-05-29T16:50:40.840Z"
   },
   {
    "duration": 3173,
    "start_time": "2022-05-29T16:51:03.279Z"
   },
   {
    "duration": 20,
    "start_time": "2022-05-29T16:51:06.454Z"
   },
   {
    "duration": 2555,
    "start_time": "2022-05-29T16:51:27.760Z"
   },
   {
    "duration": 873,
    "start_time": "2022-05-29T16:51:30.317Z"
   },
   {
    "duration": 25,
    "start_time": "2022-05-29T16:51:31.192Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-29T16:51:31.218Z"
   },
   {
    "duration": 21,
    "start_time": "2022-05-29T16:51:31.223Z"
   },
   {
    "duration": 34701,
    "start_time": "2022-05-29T16:51:31.246Z"
   },
   {
    "duration": 31,
    "start_time": "2022-05-29T16:52:05.948Z"
   },
   {
    "duration": 6064,
    "start_time": "2022-05-29T16:52:05.981Z"
   },
   {
    "duration": 59322,
    "start_time": "2022-05-29T16:52:12.047Z"
   },
   {
    "duration": 34,
    "start_time": "2022-05-29T16:53:11.370Z"
   },
   {
    "duration": 5965,
    "start_time": "2022-05-29T16:53:11.406Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-29T16:53:17.372Z"
   },
   {
    "duration": 1222,
    "start_time": "2022-05-29T16:53:17.387Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-29T16:53:18.611Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-29T16:53:18.616Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
